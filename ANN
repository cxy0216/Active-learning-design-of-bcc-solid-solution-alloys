import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from bayes_opt import BayesianOptimization
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

file_path = r'/content/屈服强度.xlsx'
data = pd.read_excel(file_path)

X = data.iloc[:, 1:-1].values
y = data.iloc[:, -1].values

scaler = StandardScaler()
X = scaler.fit_transform(X)

class RMSELoss(nn.Module):
    def __init__(self):
        super(RMSELoss, self).__init__()

    def forward(self, preds, target):
        mse = torch.mean((preds - target) ** 2) 
        return torch.sqrt(mse)

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, n_hidden, w, dropout_prob):
        super().__init__()
        layers = []
        for i in range(w):
            layers.append(nn.Linear(input_size if i == 0 else n_hidden, n_hidden))
            layers.append(nn.LayerNorm(n_hidden))  
            layers.append(nn.LeakyReLU())
            layers.append(nn.Dropout(dropout_prob))
        layers.append(nn.Linear(n_hidden, 1))
        self.model = nn.Sequential(*layers)
        self.apply(self.kaiming_init)

    def forward(self, x):
        return self.model(x)

    def kaiming_init(self, m):
        if isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

def train_model(lr, n_hidden, w, dropout_prob, weight_decay):
    torch.manual_seed(42)
    np.random.seed(42)
    n_hidden = int(n_hidden)
    w = int(w)

    kf = KFold(n_splits=len(y), shuffle=True, random_state=42)
    y_true_all = []
    y_pred_all = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        model = NeuralNetwork(input_size=X.shape[1], n_hidden=n_hidden, w=w, dropout_prob=dropout_prob).to(device)
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        criterion = RMSELoss()

        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1,1).to(device)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

        best_loss = float('inf')
        best_model_state_dict = None

        for ep in range(300):
            model.train()
            optimizer.zero_grad()
            preds = model(X_train_tensor)
            loss = criterion(preds, y_train_tensor)
            loss.backward()
            optimizer.step()

            model.eval()
            with torch.no_grad():
                preds_train = model(X_train_tensor)
                train_loss = criterion(preds_train, y_train_tensor)

            if train_loss.item() < best_loss:
                best_loss = train_loss.item()
                best_model_state_dict = model.state_dict()

        model.load_state_dict(best_model_state_dict) 
        model.eval()
        with torch.no_grad():
            preds_test = model(X_test_tensor).view(-1).cpu().numpy()
            y_pred_all.extend(preds_test)
            y_true_all.extend(y_test)

    rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))
    return -rmse

bounds = {
    'lr': (0.001, 0.05),
    'n_hidden': (16, 64),
    'w': (1, 5),
    'dropout_prob': (0.1, 0.5),
    'weight_decay': (0.00001, 0.001)
}

optimizer = BayesianOptimization(
    f=train_model,
    pbounds=bounds,
    random_state=42
)

optimizer.maximize(init_points=5, n_iter=50)

best_params = optimizer.max['params']
best_params['n_hidden'] = int(best_params['n_hidden'])
best_params['w'] = int(best_params['w'])

print("Best Parameters:", best_params)

results_df = pd.DataFrame([{
    'target': res['target'],
    **res['params']
} for res in optimizer.res])
results_df = results_df.sort_values(by='target', ascending=False)
results_df.to_excel('NeuralNetwork-Optimized-Results.xlsx', index=False)

torch.manual_seed(42)
np.random.seed(42)

kf = KFold(n_splits=len(y), shuffle=True, random_state=42)
y_true_all = []
y_pred_all = []

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

for train_idx, test_idx in kf.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    model = NeuralNetwork(
        input_size=X.shape[1],
        n_hidden=best_params['n_hidden'],
        w=best_params['w'],
        dropout_prob=best_params['dropout_prob']
    ).to(device)

    optimizer_final = optim.AdamW(
        model.parameters(),
        lr=best_params['lr'],
        weight_decay=best_params['weight_decay']
    )
    criterion = RMSELoss()

    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1,1).to(device)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

    best_train_loss = float('inf')
    best_model_state_dict = None 

    for epoch in range(300):
        model.train()
        optimizer_final.zero_grad()
        preds = model(X_train_tensor)
        loss = criterion(preds, y_train_tensor)
        loss.backward()
        optimizer_final.step()

        model.eval()
        with torch.no_grad():
            preds_train = model(X_train_tensor)
            train_loss = criterion(preds_train, y_train_tensor)

        if train_loss.item() < best_train_loss: 
            best_train_loss = train_loss.item()
            best_model_state_dict = model.state_dict()

    model.load_state_dict(best_model_state_dict) 
    model.eval()

    with torch.no_grad():
        preds_test = model(X_test_tensor).view(-1).cpu().numpy()
        y_pred_all.extend(preds_test)
        y_true_all.extend(y_test)

rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))
r2 = r2_score(y_true_all, y_pred_all)
print(f"81-Fold CV RMSE = {rmse:.4f}")
print(f"81-Fold CV R² = {r2:.4f}")

plt.figure(figsize=(6,6))
plt.scatter(y_true_all, y_pred_all, alpha=0.7)
plt.plot([min(y_true_all), max(y_true_all)], [min(y_true_all), max(y_true_all)], 'r--')
plt.xlabel("True Yield Strength")
plt.ylabel("Predicted Yield Strength")
plt.title("81-Fold CV: Predicted vs True Yield Strength (Neural Network)")
plt.grid(True)
plt.tight_layout()
plt.show()
